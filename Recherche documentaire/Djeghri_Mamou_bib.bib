
@article{deerwester_indexing_1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	issn = {0002-8231, 1097-4571},
	url = {http://doi.wiley.com/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
	doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
	pages = {391--407},
	number = {6},
	journaltitle = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	urldate = {2020-03-10},
	date = {1990-09},
	langid = {english},
	file = {Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:/home/midles/Zotero/storage/7BIX8GSY/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf}
}

@book{salton_introduction_1983,
	location = {New York},
	title = {Introduction to modern information retrieval},
	isbn = {978-0-07-054484-0},
	series = {{McGraw}-Hill computer science series},
	pagetotal = {448},
	publisher = {{McGraw}-Hill},
	author = {Salton, Gerard and {McGill}, Michael J.},
	date = {1983},
	langid = {english},
	keywords = {Information storage and retrieval systems},
	file = {Salton et McGill - 1983 - Introduction to modern information retrieval.pdf:/home/midles/Zotero/storage/ILFK8NLG/Salton et McGill - 1983 - Introduction to modern information retrieval.pdf:application/pdf}
}

@incollection{croft_simple_1994,
	location = {London},
	title = {Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval},
	isbn = {978-3-540-19889-5 978-1-4471-2099-5},
	url = {http://link.springer.com/10.1007/978-1-4471-2099-5_24},
	abstract = {The 2–Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the {TREC} test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated.},
	pages = {232--241},
	booktitle = {{SIGIR} ’94},
	publisher = {Springer London},
	author = {Robertson, S. E. and Walker, S.},
	editor = {Croft, Bruce W. and van Rijsbergen, C. J.},
	urldate = {2020-03-10},
	date = {1994},
	langid = {english},
	doi = {10.1007/978-1-4471-2099-5_24},
	file = {Robertson et Walker - 1994 - Some Simple Effective Approximations to the 2-Pois.pdf:/home/midles/Zotero/storage/7E2A3MZH/Robertson et Walker - 1994 - Some Simple Effective Approximations to the 2-Pois.pdf:application/pdf}
}

@article{lund_producing_1996,
	title = {Producing high-dimensional semantic spaces from lexical co-occurrence},
	volume = {28},
	issn = {0743-3808, 1532-5970},
	url = {http://link.springer.com/10.3758/BF03204766},
	doi = {10.3758/BF03204766},
	pages = {203--208},
	number = {2},
	journaltitle = {Behavior Research Methods, Instruments, \& Computers},
	author = {Lund, Kevin and Burgess, Curt},
	urldate = {2020-03-10},
	date = {1996-06},
	langid = {english},
	file = {Lund et Burgess - 1996 - Producing high-dimensional semantic spaces from le.pdf:/home/midles/Zotero/storage/G2MXLJFG/Lund et Burgess - 1996 - Producing high-dimensional semantic spaces from le.pdf:application/pdf}
}

@inproceedings{levy_linguistic_2014,
	location = {Ann Arbor, Michigan},
	title = {Linguistic Regularities in Sparse and Explicit Word Representations},
	url = {http://aclweb.org/anthology/W14-1618},
	doi = {10.3115/v1/W14-1618},
	abstract = {Recent work has shown that neuralembedded word representations capture many relational similarities, which can be recovered by means of vector arithmetic in the embedded space. We show that Mikolov et al.’s method of ﬁrst adding and subtracting word vectors, and then searching for a word similar to the result, is equivalent to searching for a word that maximizes a linear combination of three pairwise word similarities. Based on this observation, we suggest an improved method of recovering relational similarities, improving the state-of-the-art results on two recent word-analogy datasets. Moreover, we demonstrate that analogy recovery is not restricted to neural word embeddings, and that a similar amount of relational similarities can be recovered from traditional distributional word representations.},
	eventtitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
	pages = {171--180},
	booktitle = {Proceedings of the Eighteenth Conference on Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Levy, Omer and Goldberg, Yoav},
	urldate = {2020-03-10},
	date = {2014},
	langid = {english},
	file = {Levy et Goldberg - 2014 - Linguistic Regularities in Sparse and Explicit Wor.pdf:/home/midles/Zotero/storage/6ZMMZVAL/Levy et Goldberg - 2014 - Linguistic Regularities in Sparse and Explicit Wor.pdf:application/pdf}
}

@article{rohde_improved_nodate,
	title = {An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence},
	abstract = {The lexical semantic system is an important component of human language and cognitive processing. One approach to modeling semantic knowledge makes use of hand-constructed networks or trees of interconnected word senses (Miller, Beckwith, Fellbaum, Gross, \& Miller, 1990; Jarmasz \& Szpakowicz, 2003). An alternative approach seeks to model word meanings as high-dimensional vectors, which are derived from the cooccurrence of words in unlabeled text corpora (Landauer \& Dumais, 1997; Burgess \& Lund, 1997a). This paper introduces a new vector-space method for deriving word-meanings from large corpora that was inspired by the {HAL} and {LSA} models, but which achieves better and more consistent results in predicting human similarity judgments. We explain the new model, known as {COALS}, and how it relates to prior methods, and then evaluate the various models on a range of tasks, including a novel set of semantic similarity ratings involving both semantically and morphologically related terms.},
	pages = {33},
	author = {Rohde, Douglas L T and Gonnerman, Laura M and Plaut, David C},
	langid = {english},
	file = {Rohde et al. - An Improved Model of Semantic Similarity Based on .pdf:/home/midles/Zotero/storage/4YLV4LHP/Rohde et al. - An Improved Model of Semantic Similarity Based on .pdf:application/pdf}
}

@inproceedings{baroni_dont_2014,
	location = {Baltimore, Maryland},
	title = {Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors},
	url = {http://aclweb.org/anthology/P14-1023},
	doi = {10.3115/v1/P14-1023},
	abstract = {Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justiﬁed, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts.},
	eventtitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {238--247},
	booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Baroni, Marco and Dinu, Georgiana and Kruszewski, Germán},
	urldate = {2020-03-10},
	date = {2014},
	langid = {english},
	file = {Baroni et al. - 2014 - Don't count, predict! A systematic comparison of c.pdf:/home/midles/Zotero/storage/CQJLNAPU/Baroni et al. - 2014 - Don't count, predict! A systematic comparison of c.pdf:application/pdf}
}

@article{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	journaltitle = {{arXiv}:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	urldate = {2020-03-10},
	date = {2013-09-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:/home/midles/Zotero/storage/99IWWZV9/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf}
}

@article{cosijn_dimensions_2000,
	title = {Dimensions of relevance},
	volume = {36},
	issn = {03064573},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457399000722},
	doi = {10.1016/S0306-4573(99)00072-2},
	abstract = {Relevance has become a major area of research in the ®eld of Information Retrieval, despite the fact that the concept relevance is not well understood. This paper models manifestations of relevance within a system of relevance attributes to show that the attributes function in dierent dimensions for the dierent manifestations of relevance. It is shown that motivational relevance, as a manifestation of relevance, should not be viewed as part of a linear, objective±subjective scale of relevances, but rather as an attribute of relevance. Similarly, that the manifestation of aective relevance should not be viewed as a discrete category of relevance manifestation, but rather as an in¯uencing factor on the other subjective relevance types. The paper argues a consolidated model of relevance manifestations which includes the notion of socio-cognitive relevance. 7 2000 Elsevier Science Ltd. All rights reserved.},
	pages = {533--550},
	number = {4},
	journaltitle = {Information Processing \& Management},
	author = {Cosijn, Erica and Ingwersen, Peter},
	urldate = {2020-03-10},
	date = {2000-07},
	langid = {english},
	file = {Cosijn et Ingwersen - 2000 - Dimensions of relevance.pdf:/home/midles/Zotero/storage/VMTRNV5J/Cosijn et Ingwersen - 2000 - Dimensions of relevance.pdf:application/pdf}
}

@article{rocchio_xxiii_nodate,
	title = {{XXIII}. {RELEVANCE} {FEEDBACK} {IN} {INFORMATION} {RETRIEVAL}},
	abstract = {In evaluating the performance of a document retrieval system one must attempt to isolate the critical variables which determine system behavior. For this purpose a model is introduced which identifies indexing, search request formulation,, and request-document matching as the three primary functions of an automatic retrieval system. Search request formulation,, the responsibility of the users of the system, Is considered to be the variable with the greatest potential variance. In view of this, the idea of the optimization of search requests is believed to constitute a primary possibility toward better control In evaluating indexing and request-document matching. Investigation into request optimality leads In turn to a method for improving search requests in an operational framework. For this purpose the notion of interaction between a user and an information retrieval system by means of relevance feedback is introduced. A process of request modifications is developed based on a sequence of retrieval operations, such that after each operation the user Is allowed to communicate his evaluation to the system. This information Is used as a basis for altering the user's query. The modification algorithm is developed below and some preliminary results are presented.},
	pages = {18},
	author = {Rocchio, J},
	langid = {english},
	file = {Rocchio - XXIII. RELEVANCE FEEDBACK IN INFORMATION RETRIEVAL.pdf:/home/midles/Zotero/storage/BIQMTHAM/Rocchio - XXIII. RELEVANCE FEEDBACK IN INFORMATION RETRIEVAL.pdf:application/pdf}
}

@inproceedings{cao_selecting_2008,
	location = {Singapore, Singapore},
	title = {Selecting good expansion terms for pseudo-relevance feedback},
	isbn = {978-1-60558-164-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390334.1390377},
	doi = {10.1145/1390334.1390377},
	abstract = {Pseudo-relevance feedback assumes that most frequent terms in the pseudo-feedback documents are useful for the retrieval. In this study, we re-examine this assumption and show that it does not hold in reality – many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. We also show that good expansion terms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. We then propose to integrate a term classification process to predict the usefulness of expansion terms. Multiple additional features can be integrated in this process. Our experiments on three {TREC} collections show that retrieval effectiveness can be much improved when term classification is used. In addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning.},
	eventtitle = {the 31st annual international {ACM} {SIGIR} conference},
	pages = {243},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval - {SIGIR} '08},
	publisher = {{ACM} Press},
	author = {Cao, Guihong and Nie, Jian-Yun and Gao, Jianfeng and Robertson, Stephen},
	urldate = {2020-03-10},
	date = {2008},
	langid = {english},
	file = {Cao et al. - 2008 - Selecting good expansion terms for pseudo-relevanc.pdf:/home/midles/Zotero/storage/TY3FJDTC/Cao et al. - 2008 - Selecting good expansion terms for pseudo-relevanc.pdf:application/pdf}
}

@article{berger_information_2017,
	title = {Information Retrieval as Statistical Translation},
	volume = {51},
	abstract = {We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation. The central ingredient in this approach is a statistical model of how a user might distill or translate" a given document into a query. To assess the relevance of a document to a user's query, we estimate the probability that the query would have been generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents. We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justi cation of the language modeling" strategy recently proposed by Ponte and Croft. In a series of experiments on {TREC} data, a simple translation-based retrieval system performs well in comparison to conventional retrieval techniques. This prototype system only begins to tap the full potential of translation-based retrieval.},
	pages = {8},
	number = {2},
	journaltitle = {{ACM} {SIGIR} Forum},
	author = {Berger, Adam},
	date = {2017},
	langid = {english},
	file = {Berger - 2017 - Information Retrieval as Statistical Translation.pdf:/home/midles/Zotero/storage/6DJVKF38/Berger - 2017 - Information Retrieval as Statistical Translation.pdf:application/pdf}
}

@online{noauthor_pdf_nodate,
	title = {({PDF}) Estimation of statistical translation models based on mutual information for ad hoc information retrieval {\textbar} {ChengXiang} Zhai - Academia.edu},
	url = {https://www.academia.edu/2785732/Estimation_of_statistical_translation_models_based_on_mutual_information_for_ad_hoc_information_retrieval},
	urldate = {2020-03-10},
	file = {(PDF) Estimation of statistical translation models based on mutual information for ad hoc information retrieval | ChengXiang Zhai - Academia.edu:/home/midles/Zotero/storage/B7XVGXFK/Estimation_of_statistical_translation_models_based_on_mutual_information_for_ad_hoc_information.html:text/html}
}

@report{abdul-jaleel_umass_2004,
	location = {Fort Belvoir, {VA}},
	title = {{UMass} at {TREC} 2004: Novelty and {HARD}:},
	url = {http://www.dtic.mil/docs/citations/ADA460118},
	shorttitle = {{UMass} at {TREC} 2004},
	institution = {Defense Technical Information Center},
	author = {Abdul-Jaleel, Nasreen and Allan, James and Croft, W. B. and Diaz, Fernando and Larkey, Leah and Li, Xiaoyan and Smucker, Mark D. and Wade, Courtney},
	urldate = {2020-03-10},
	date = {2004-01-01},
	langid = {english},
	doi = {10.21236/ADA460118},
	file = {Abdul-Jaleel et al. - 2004 - UMass at TREC 2004 Novelty and HARD.pdf:/home/midles/Zotero/storage/YDK63RH5/Abdul-Jaleel et al. - 2004 - UMass at TREC 2004 Novelty and HARD.pdf:application/pdf}
}

@article{lavrenko_relevance-based_nodate,
	title = {Relevance-Based Language Models},
	abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate a relevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on {TREC} retrieval and {TDT} tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
	pages = {8},
	author = {Lavrenko, Victor and Croft, W Bruce},
	langid = {english},
	file = {Lavrenko et Croft - Relevance-Based Language Models.pdf:/home/midles/Zotero/storage/78P66I9U/Lavrenko et Croft - Relevance-Based Language Models.pdf:application/pdf}
}

@article{nakamoto_foreword_2011,
	title = {{FOREWORD}},
	volume = {E94-D},
	issn = {0916-8532, 1745-1361},
	url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/transinf/E94.D.1?from=CrossRef},
	doi = {10.1587/transinf.E94.D.1},
	abstract = {Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining. Intensive studies have been conducted on the problem and signiﬁcant progress has been made [1], [2]. This short paper gives an introduction to learning to rank, and it speciﬁcally explains the fundamental problems, existing approaches, and future work of learning to rank. Several learning to rank methods using {SVM} techniques are described in details.},
	pages = {1--2},
	number = {1},
	journaltitle = {{IEICE} Transactions on Information and Systems},
	author = {Nakamoto, Yukikazu},
	urldate = {2020-03-10},
	date = {2011},
	langid = {english},
	file = {Nakamoto - 2011 - FOREWORD.pdf:/home/midles/Zotero/storage/DJQKULRH/Nakamoto - 2011 - FOREWORD.pdf:application/pdf}
}

@book{liu_learning_2011,
	location = {Berlin, Heidelberg},
	title = {Learning to Rank for Information Retrieval},
	isbn = {978-3-642-14266-6 978-3-642-14267-3},
	url = {http://link.springer.com/10.1007/978-3-642-14267-3},
	publisher = {Springer Berlin Heidelberg},
	author = {Liu, Tie-Yan},
	urldate = {2020-03-10},
	date = {2011},
	langid = {english},
	doi = {10.1007/978-3-642-14267-3},
	file = {Liu - 2011 - Learning to Rank for Information Retrieval.pdf:/home/midles/Zotero/storage/QKAVQJF2/Liu - 2011 - Learning to Rank for Information Retrieval.pdf:application/pdf}
}

@inproceedings{cossock_subset_2006,
	location = {Berlin, Heidelberg},
	title = {Subset Ranking Using Regression},
	isbn = {978-3-540-35296-9},
	doi = {10.1007/11776420_44},
	series = {Lecture Notes in Computer Science},
	abstract = {We study the subset ranking problem, motivated by its important application in web-search. In this context, we consider the standard {DCG} criterion (discounted cumulated gain) that measures the quality of items near the top of the rank-list. Similar to error minimization for binary classification, the {DCG} criterion leads to a non-convex optimization problem that can be {NP}-hard. Therefore a computationally more tractable approach is needed. We present bounds that relate the approximate optimization of {DCG} to the approximate minimization of certain regression errors. These bounds justify the use of convex learning formulations for solving the subset ranking problem. The resulting estimation methods are not conventional, in that we focus on the estimation quality in the top-portion of the rank-list. We further investigate the generalization ability of these formulations. Under appropriate conditions, the consistency of the estimation schemes with respect to the {DCG} metric can be derived.},
	pages = {605--619},
	booktitle = {Learning Theory},
	publisher = {Springer},
	author = {Cossock, David and Zhang, Tong},
	editor = {Lugosi, Gábor and Simon, Hans Ulrich},
	date = {2006},
	langid = {english}
}

@article{li_mcrank_nodate,
	title = {{McRank}: Learning to Rank Using Multiple Classification and Gradient Boosting},
	abstract = {We cast the ranking problem as (1) multiple classiﬁcation (“Mc”) (2) multiple ordinal classiﬁcation, which lead to computationally tractable learning algorithms for relevance ranking in Web search. We consider the {DCG} criterion (discounted cumulative gain), a standard quality measure in information retrieval. Our approach is motivated by the fact that perfect classiﬁcations result in perfect {DCG} scores and the {DCG} errors are bounded by classiﬁcation errors. We propose using the Expected Relevance to convert class probabilities into ranking scores. The class probabilities are learned using a gradient boosting tree algorithm. Evaluations on large-scale datasets show that our approach can improve {LambdaRank} [5] and the regressions-based ranker [6], in terms of the (normalized) {DCG} scores. An efﬁcient implementation of the boosting tree algorithm is also presented.},
	pages = {8},
	author = {Li, Ping and Wu, Qiang and Burges, Christopher J},
	langid = {english},
	file = {Li et al. - McRank Learning to Rank Using Multiple Classifica.pdf:/home/midles/Zotero/storage/A6X83K46/Li et al. - McRank Learning to Rank Using Multiple Classifica.pdf:application/pdf}
}

@incollection{shashua_ranking_2003,
	title = {Ranking with Large Margin Principle: Two Approaches},
	url = {http://papers.nips.cc/paper/2269-ranking-with-large-margin-principle-two-approaches.pdf},
	shorttitle = {Ranking with Large Margin Principle},
	pages = {961--968},
	booktitle = {Advances in Neural Information Processing Systems 15},
	publisher = {{MIT} Press},
	author = {Shashua, Amnon and Levin, Anat},
	editor = {Becker, S. and Thrun, S. and Obermayer, K.},
	urldate = {2020-03-10},
	date = {2003},
	file = {NIPS Full Text PDF:/home/midles/Zotero/storage/TMM35VX7/Shashua et Levin - 2003 - Ranking with Large Margin Principle Two Approache.pdf:application/pdf;NIPS Snapshot:/home/midles/Zotero/storage/Q9558XN4/2269-ranking-with-large-margin-principle-two-approaches.html:text/html}
}

@inproceedings{burges_learning_2005,
	location = {Bonn, Germany},
	title = {Learning to rank using gradient descent},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102363},
	doi = {10.1145/1102351.1102363},
	abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce {RankNet}, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
	eventtitle = {the 22nd international conference},
	pages = {89--96},
	booktitle = {Proceedings of the 22nd international conference on Machine learning  - {ICML} '05},
	publisher = {{ACM} Press},
	author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
	urldate = {2020-03-10},
	date = {2005},
	langid = {english},
	file = {Burges et al. - 2005 - Learning to rank using gradient descent.pdf:/home/midles/Zotero/storage/ASZ27LDS/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:application/pdf}
}

@incollection{burges_learning_2007,
	title = {Learning to Rank with Nonsmooth Cost Functions},
	url = {http://papers.nips.cc/paper/2971-learning-to-rank-with-nonsmooth-cost-functions.pdf},
	pages = {193--200},
	booktitle = {Advances in Neural Information Processing Systems 19},
	publisher = {{MIT} Press},
	author = {Burges, Christopher J. and Ragno, Robert and Le, Quoc V.},
	editor = {Schölkopf, B. and Platt, J. C. and Hoffman, T.},
	urldate = {2020-03-10},
	date = {2007},
	file = {NIPS Full Text PDF:/home/midles/Zotero/storage/FX2S5Q6W/Burges et al. - 2007 - Learning to Rank with Nonsmooth Cost Functions.pdf:application/pdf;NIPS Snapshot:/home/midles/Zotero/storage/W9LM73YZ/2971-learning-to-rank-with-nonsmooth-cost-functions.html:text/html}
}

@article{wu_adapting_2010,
	title = {Adapting boosting for information retrieval measures},
	volume = {13},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/10.1007/s10791-009-9112-1},
	doi = {10.1007/s10791-009-9112-1},
	abstract = {We present a new ranking algorithm that combines the strengths of two previous methods: boosted tree classiﬁcation, and {LambdaRank}, which has been shown to be empirically optimal for a widely used information retrieval measure. Our algorithm is based on boosted regression trees, although the ideas apply to any weak learners, and it is signiﬁcantly faster in both train and test phases than the state of the art, for comparable accuracy. We also show how to ﬁnd the optimal linear combination for any two rankers, and we use this method to solve the line search problem exactly during boosting. In addition, we show that starting with a previously trained model, and boosting using its residuals, furnishes an eﬀective technique for model adaptation, and we give signiﬁcantly improved results for a particularly pressing problem in Web Search - training rankers for markets for which only small amounts of labeled data are available, given a ranker trained on much more data from a larger market.},
	pages = {254--270},
	number = {3},
	journaltitle = {Information Retrieval},
	author = {Wu, Qiang and Burges, Christopher J. C. and Svore, Krysta M. and Gao, Jianfeng},
	urldate = {2020-03-10},
	date = {2010-06},
	langid = {english},
	file = {Wu et al. - 2010 - Adapting boosting for information retrieval measur.pdf:/home/midles/Zotero/storage/8U32TR5K/Wu et al. - 2010 - Adapting boosting for information retrieval measur.pdf:application/pdf}
}

@inproceedings{cao_learning_2007,
	location = {Corvalis, Oregon},
	title = {Learning to rank: from pairwise approach to listwise approach},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273513},
	doi = {10.1145/1273496.1273513},
	shorttitle = {Learning to rank},
	abstract = {The paper is concerned with learning to rank, which is to construct a model or a function for ranking objects. Learning to rank is useful for document retrieval, collaborative ﬁltering, and many other applications. Several methods for learning to rank have been proposed, which take object pairs as ‘instances’ in learning. We refer to them as the pairwise approach in this paper. Although the pairwise approach oﬀers advantages, it ignores the fact that ranking is a prediction task on list of objects. The paper postulates that learning to rank should adopt the listwise approach in which lists of objects are used as ‘instances’ in learning. The paper proposes a new probabilistic method for the approach. Speciﬁcally it introduces two probability models, respectively referred to as permutation probability and top one probability, to deﬁne a listwise loss function for learning. Neural Network and Gradient Descent are then employed as model and algorithm in the learning method. Experimental results on information retrieval show that the proposed listwise approach performs better than the pairwise approach.},
	eventtitle = {the 24th international conference},
	pages = {129--136},
	booktitle = {Proceedings of the 24th international conference on Machine learning - {ICML} '07},
	publisher = {{ACM} Press},
	author = {Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},
	urldate = {2020-03-10},
	date = {2007},
	langid = {english},
	file = {Cao et al. - 2007 - Learning to rank from pairwise approach to listwi.pdf:/home/midles/Zotero/storage/PP84FERZ/Cao et al. - 2007 - Learning to rank from pairwise approach to listwi.pdf:application/pdf}
}

@article{xu_adarank_2007,
	title = {{AdaRank}: A Boosting Algorithm for Information Retrieval},
	abstract = {In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance measures such as {MAP} (Mean Average Precision) and {NDCG} (Normalized Discounted Cumulative Gain). Ideally a learning algorithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by minimizing loss functions loosely related to the performance measures. For example, Ranking {SVM} and {RankBoost} train ranking models by minimizing classiﬁcation errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly deﬁned on the performance measures. Our algorithm, referred to as {AdaRank}, repeatedly constructs ‘weak rankers’ on the basis of re-weighted training data and ﬁnally linearly combines the weak rankers for making ranking predictions. We prove that the training process of {AdaRank} is exactly that of enhancing the performance measure used. Experimental results on four benchmark datasets show that {AdaRank} signiﬁcantly outperforms the baseline methods of {BM}25, Ranking {SVM}, and {RankBoost}.},
	pages = {8},
	author = {Xu, Jun and Li, Hang},
	date = {2007},
	langid = {english},
	file = {Xu et Li - 2007 - AdaRank A Boosting Algorithm for Information Retr.pdf:/home/midles/Zotero/storage/66W5BRYT/Xu et Li - 2007 - AdaRank A Boosting Algorithm for Information Retr.pdf:application/pdf}
}

@inproceedings{yue_support_2007,
	location = {Amsterdam, The Netherlands},
	title = {A support vector method for optimizing average precision},
	isbn = {978-1-59593-597-7},
	url = {http://portal.acm.org/citation.cfm?doid=1277741.1277790},
	doi = {10.1145/1277741.1277790},
	abstract = {Machine learning is commonly used to improve ranked retrieval systems. Due to computational diﬃculties, few learning techniques have been developed to directly optimize for mean average precision ({MAP}), despite its widespread use in evaluating such systems. Existing approaches optimizing {MAP} either do not ﬁnd a globally optimal solution, or are computationally expensive. In contrast, we present a general {SVM} learning algorithm that eﬃciently ﬁnds a globally optimal solution to a straightforward relaxation of {MAP}. We evaluate our approach using the {TREC} 9 and {TREC} 10 Web Track corpora ({WT}10g), comparing against {SVMs} optimized for accuracy and {ROCArea}. In most cases we show our method to produce statistically signiﬁcant improvements in {MAP} scores.},
	eventtitle = {the 30th annual international {ACM} {SIGIR} conference},
	pages = {271},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval - {SIGIR} '07},
	publisher = {{ACM} Press},
	author = {Yue, Yisong and Finley, Thomas and Radlinski, Filip and Joachims, Thorsten},
	urldate = {2020-03-10},
	date = {2007},
	langid = {english},
	file = {Yue et al. - 2007 - A support vector method for optimizing average pre.pdf:/home/midles/Zotero/storage/5WYBTQRZ/Yue et al. - 2007 - A support vector method for optimizing average pre.pdf:application/pdf}
}

@article{harris_distributional_1954,
	title = {Distributional Structure},
	volume = {10},
	issn = {0043-7956, 2373-5112},
	url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
	doi = {10.1080/00437956.1954.11659520},
	pages = {146--162},
	number = {2},
	journaltitle = {\textit{{WORD}}},
	author = {Harris, Zellig S.},
	urldate = {2020-03-15},
	date = {1954-08},
	langid = {english},
	file = {Harris - 1954 - Distributional Structure.pdf:/home/midles/Zotero/storage/3C2E2ZK3/Harris - 1954 - Distributional Structure.pdf:application/pdf}
}

@article{mitra_neural_2017,
	title = {Neural Models for Information Retrieval},
	url = {http://arxiv.org/abs/1705.01509},
	abstract = {Neural ranking models for information retrieval ({IR}) use shallow or deep neural networks to rank search results in response to a query. Traditional learning to rank models employ machine learning techniques over hand-crafted {IR} features. By contrast, neural models learn representations of language from raw text that can bridge the gap between query and document vocabulary. Unlike classical {IR} models, these new machine learning based approaches are data-hungry, requiring large scale training data before they can be deployed. This tutorial introduces basic concepts and intuitions behind neural {IR} models, and places them in the context of traditional retrieval models. We begin by introducing fundamental concepts of {IR} and different neural and non-neural approaches to learning vector representations of text. We then review shallow neural {IR} methods that employ pre-trained neural term embeddings without learning the {IR} task end-to-end. We introduce deep neural networks next, discussing popular deep architectures. Finally, we review the current {DNN} models for information retrieval. We conclude with a discussion on potential future directions for neural {IR}.},
	journaltitle = {{arXiv}:1705.01509 [cs]},
	author = {Mitra, Bhaskar and Craswell, Nick},
	urldate = {2020-03-15},
	date = {2017-05-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1705.01509},
	keywords = {Computer Science - Information Retrieval},
	file = {Mitra et Craswell - 2017 - Neural Models for Information Retrieval.pdf:/home/midles/Zotero/storage/DEPN74JB/Mitra et Craswell - 2017 - Neural Models for Information Retrieval.pdf:application/pdf}
}

@article{turney_frequency_2010,
	title = {From Frequency to Meaning: Vector Space Models of Semantics},
	volume = {37},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10640},
	doi = {10.1613/jair.2934},
	shorttitle = {From Frequency to Meaning},
	abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models ({VSMs}) of semantics are beginning to address these limits. This paper surveys the use of {VSMs} for semantic processing of text. We organize the literature on {VSMs} according to the structure of the matrix in a {VSM}. There are currently three broad classes of {VSMs}, based on term–document, word–context, and pair–pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a speciﬁc open source project in each category. Our goal in this survey is to show the breadth of applications of {VSMs} for semantics, to provide a new perspective on {VSMs} for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the ﬁeld.},
	pages = {141--188},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Turney, P. D. and Pantel, P.},
	urldate = {2020-03-15},
	date = {2010-02-27},
	langid = {english},
	file = {Turney et Pantel - 2010 - From Frequency to Meaning Vector Space Models of .pdf:/home/midles/Zotero/storage/APS5KXZE/Turney et Pantel - 2010 - From Frequency to Meaning Vector Space Models of .pdf:application/pdf}
}

@article{golub_singular_nodate,
	title = {Singular value decomposition and least squares solutions},
	pages = {18},
	author = {Golub, G H and Reinsch, C},
	langid = {english},
	file = {Golub et Reinsch - Singular value decomposition and least squares sol.pdf:/home/midles/Zotero/storage/RKVRFC4K/Golub et Reinsch - Singular value decomposition and least squares sol.pdf:application/pdf}
}

@article{dempster_maximum_1977,
	title = {Maximum Likelihood from Incomplete Data Via the \textit{{EM}} Algorithm},
	volume = {39},
	issn = {00359246},
	url = {http://doi.wiley.com/10.1111/j.2517-6161.1977.tb01600.x},
	doi = {10.1111/j.2517-6161.1977.tb01600.x},
	pages = {1--22},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	urldate = {2020-03-15},
	date = {1977-09},
	langid = {english},
	file = {Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data Via the i.pdf:/home/midles/Zotero/storage/3VRAW8XE/Dempster et al. - 1977 - Maximum Likelihood from Incomplete Data Via the i.pdf:application/pdf}
}

@article{hofmann_probabilistic_2017,
	title = {Probabilistic Latent Semantic Indexing},
	volume = {51},
	abstract = {Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain speci c synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing {LSI} by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and de nes a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over {LSI}. In particular, the combination of models with di erent dimensionalities has proven to be advantageous.},
	pages = {8},
	number = {2},
	journaltitle = {{ACM} {SIGIR} Forum},
	author = {Hofmann, Thomas},
	date = {2017},
	langid = {english},
	file = {Hofmann - 2017 - Probabilistic Latent Semantic Indexing.pdf:/home/midles/Zotero/storage/PHZ4AY9P/Hofmann - 2017 - Probabilistic Latent Semantic Indexing.pdf:application/pdf}
}

@article{galeshchuk_modelisation_nodate,
	title = {Modélisation thématique à l’aide des plongements lexicaux issus de Word2Vec},
	abstract = {This paper discusses approaches for static topic modeling, in particular an improved method based on topic parametrization from a continuous distribution over the space of word embeddings. Word embeddings corpora proves to reﬂect semantic interdependences. Thus, we incorporate vectorized word representations trained with Word2Vec neural network in a generative process of topic modeling. The alternative approach with beta approximation of mutual information distribution over embeddings is proposed and compared with vanilla {LDA} and Gaussian {LDA} methods.},
	pages = {5},
	author = {Galeshchuk, Svitlana and Chaves, Bruno},
	langid = {french},
	file = {Galeshchuk et Chaves - Modélisation thématique à l’aide des plongements l.pdf:/home/midles/Zotero/storage/VPFZ3YCY/Galeshchuk et Chaves - Modélisation thématique à l’aide des plongements l.pdf:application/pdf}
}

@article{blei_latent_nodate,
	title = {Latent Dirichlet Allocation},
	abstract = {We describe latent Dirichlet allocation ({LDA}), a generative probabilistic model for collections of discrete data such as text corpora. {LDA} is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a ﬁnite mixture over an underlying set of topics. Each topic is, in turn, modeled as an inﬁnite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efﬁcient approximate inference techniques based on variational methods and an {EM} algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation, and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic {LSI} model.},
	pages = {30},
	author = {Blei, David M},
	langid = {english},
	file = {Blei - Latent Dirichlet Allocation.pdf:/home/midles/Zotero/storage/BUCPAVIU/Blei - Latent Dirichlet Allocation.pdf:application/pdf}
}

@article{francesiaz_introduction_nodate,
	title = {Introduction aux modèles probabilistes utilisés en Fouille de Données},
	pages = {27},
	author = {Francesiaz, Théo and Graille, Raphaël and Metahri, Brahim},
	langid = {french},
	file = {Francesiaz et al. - Introduction aux modèles probabilistes utilisés en.pdf:/home/midles/Zotero/storage/BFKEH8Q9/Francesiaz et al. - Introduction aux modèles probabilistes utilisés en.pdf:application/pdf}
}

@inproceedings{pennington_glove_2014,
	location = {Doha, Qatar},
	title = {Glove: Global Vectors for Word Representation},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	shorttitle = {Glove},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	eventtitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	pages = {1532--1543},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	urldate = {2020-03-15},
	date = {2014},
	langid = {english},
	file = {Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:/home/midles/Zotero/storage/3QYZLM9Y/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf:application/pdf}
}

@article{le_distributed_nodate,
	title = {Distributed Representations of Sentences and Documents},
	abstract = {Many machine learning algorithms require the input to be represented as a ﬁxed-length feature vector. When it comes to texts, one of the most common ﬁxed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, “powerful,” “strong” and “Paris” are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns ﬁxed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-ofwords models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classiﬁcation and sentiment analysis tasks.},
	pages = {9},
	author = {Le, Quoc and Mikolov, Tomas},
	langid = {english},
	file = {Le et Mikolov - Distributed Representations of Sentences and Docum.pdf:/home/midles/Zotero/storage/XHCIFLHU/Le et Mikolov - Distributed Representations of Sentences and Docum.pdf:application/pdf}
}

@inproceedings{huang_learning_2013,
	location = {San Francisco, California, {USA}},
	title = {Learning deep structured semantic models for web search using clickthrough data},
	isbn = {978-1-4503-2263-8},
	url = {http://dl.acm.org/citation.cfm?doid=2505515.2505665},
	doi = {10.1145/2505515.2505665},
	abstract = {Latent semantic models, such as {LSA}, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.},
	eventtitle = {the 22nd {ACM} international conference},
	pages = {2333--2338},
	booktitle = {Proceedings of the 22nd {ACM} international conference on Conference on information \& knowledge management - {CIKM} '13},
	publisher = {{ACM} Press},
	author = {Huang, Po-Sen and He, Xiaodong and Gao, Jianfeng and Deng, Li and Acero, Alex and Heck, Larry},
	urldate = {2020-03-15},
	date = {2013},
	langid = {english},
	file = {Huang et al. - 2013 - Learning deep structured semantic models for web s.pdf:/home/midles/Zotero/storage/8WUZPRAG/Huang et al. - 2013 - Learning deep structured semantic models for web s.pdf:application/pdf}
}

@inproceedings{zuccon_integrating_2015,
	location = {Parramatta, {NSW}, Australia},
	title = {Integrating and Evaluating Neural Word Embeddings in Information Retrieval},
	isbn = {978-1-4503-4040-3},
	url = {http://dl.acm.org/citation.cfm?doid=2838931.2838936},
	doi = {10.1145/2838931.2838936},
	abstract = {Recent advances in neural language models have contributed new methods for learning distributed vector representations of words (also called word embeddings). Two such methods are the continuous bag-of-words model and the skipgram model. These methods have been shown to produce embeddings that capture higher order relationships between words that are highly eﬀective in natural language processing tasks involving the use of word similarity and word analogy. Despite these promising results, there has been little analysis of the use of these word embeddings for retrieval.},
	eventtitle = {the 20th Australasian Document Computing Symposium},
	pages = {1--8},
	booktitle = {Proceedings of the 20th Australasian Document Computing Symposium on {ZZZ} - {ADCS} '15},
	publisher = {{ACM} Press},
	author = {Zuccon, Guido and Koopman, Bevan and Bruza, Peter and Azzopardi, Leif},
	urldate = {2020-03-15},
	date = {2015},
	langid = {english},
	file = {Zuccon et al. - 2015 - Integrating and Evaluating Neural Word Embeddings .pdf:/home/midles/Zotero/storage/N2EL79M4/Zuccon et al. - 2015 - Integrating and Evaluating Neural Word Embeddings .pdf:application/pdf}
}

@inproceedings{shen_learning_2014,
	location = {Seoul, Korea},
	title = {Learning semantic representations using convolutional neural networks for web search},
	isbn = {978-1-4503-2745-9},
	url = {http://dl.acm.org/citation.cfm?doid=2567948.2577348},
	doi = {10.1145/2567948.2577348},
	abstract = {This paper presents a series of new latent semantic models based on a convolutional neural network ({CNN}) to learn lowdimensional semantic vectors for search queries and Web documents. By using the convolution-max pooling operation, local contextual information at the word n-gram level is modeled first. Then, salient local features in a word sequence are combined to form a global feature vector. Finally, the high-level semantic information of the word sequence is extracted to form a global vector representation. The proposed models are trained on clickthrough data by maximizing the conditional likelihood of clicked documents given a query, using stochastic gradient ascent. The new models are evaluated on a Web document ranking task using a large-scale, real-world data set. Results show that our model significantly outperforms other semantic models, which were state-of-the-art in retrieval performance prior to this work.},
	eventtitle = {the 23rd International Conference},
	pages = {373--374},
	booktitle = {Proceedings of the 23rd International Conference on World Wide Web - {WWW} '14 Companion},
	publisher = {{ACM} Press},
	author = {Shen, Yelong and He, Xiaodong and Gao, Jianfeng and Deng, Li and Mesnil, Grégoire},
	urldate = {2020-03-15},
	date = {2014},
	langid = {english},
	file = {Shen et al. - 2014 - Learning semantic representations using convolutio.pdf:/home/midles/Zotero/storage/KDLIQVDU/Shen et al. - 2014 - Learning semantic representations using convolutio.pdf:application/pdf}
}

@article{lu_deep_nodate,
	title = {A Deep Architecture for Matching Short Texts},
	abstract = {Many machine learning problems can be interpreted as learning for matching two types of objects (e.g., images and captions, users and products, queries and documents, etc.). The matching level of two objects is usually measured as the inner product in a certain feature space, while the modeling effort focuses on mapping of objects from the original space to the feature space. This schema, although proven successful on a range of matching tasks, is insufﬁcient for capturing the rich structure in the matching process of more complicated objects. In this paper, we propose a new deep architecture to more effectively model the complicated matching relations between two objects from heterogeneous domains. More speciﬁcally, we apply this model to matching tasks in natural language, e.g., ﬁnding sensible responses for a tweet, or relevant answers to a given question. This new architecture naturally combines the localness and hierarchy intrinsic to the natural language problems, and therefore greatly improves upon the state-of-the-art models.},
	pages = {9},
	author = {Lu, Zhengdong and Li, Hang},
	langid = {english},
	file = {Lu et Li - A Deep Architecture for Matching Short Texts.pdf:/home/midles/Zotero/storage/A67LWHGX/Lu et Li - A Deep Architecture for Matching Short Texts.pdf:application/pdf}
}

@article{guo_deep_2016,
	title = {A Deep Relevance Matching Model for Ad-hoc Retrieval},
	url = {http://arxiv.org/abs/1711.08611},
	doi = {10.1145/2983323.2983769},
	abstract = {In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing ({NLP}) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many {NLP} tasks such as paraphrase identiﬁcation, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most {NLP} matching tasks concern semantic matching, and there are some fundamental diﬀerences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model ({DRMM}) for ad-hoc retrieval. Speciﬁcally, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can eﬀectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can signiﬁcantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.},
	pages = {55--64},
	journaltitle = {Proceedings of the 25th {ACM} International on Conference on Information and Knowledge Management - {CIKM} '16},
	author = {Guo, Jiafeng and Fan, Yixing and Ai, Qingyao and Croft, W. Bruce},
	urldate = {2020-03-15},
	date = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.08611},
	keywords = {Computer Science - Information Retrieval},
	file = {Guo et al. - 2016 - A Deep Relevance Matching Model for Ad-hoc Retriev.pdf:/home/midles/Zotero/storage/MF9TWZGX/Guo et al. - 2016 - A Deep Relevance Matching Model for Ad-hoc Retriev.pdf:application/pdf}
}

@article{hu_convolutional_2015,
	title = {Convolutional Neural Network Architectures for Matching Natural Language Sentences},
	url = {http://arxiv.org/abs/1503.03244},
	abstract = {Semantic matching is of central importance to many natural language tasks {\textbackslash}cite\{bordes2014semantic,{RetrievalQA}\}. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layer-by-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.},
	journaltitle = {{arXiv}:1503.03244 [cs]},
	author = {Hu, Baotian and Lu, Zhengdong and Li, Hang and Chen, Qingcai},
	urldate = {2020-03-15},
	date = {2015-03-11},
	eprinttype = {arxiv},
	eprint = {1503.03244},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/midles/Zotero/storage/Y9YBDLV2/Hu et al. - 2015 - Convolutional Neural Network Architectures for Mat.pdf:application/pdf;arXiv.org Snapshot:/home/midles/Zotero/storage/4SHN8QKR/1503.html:text/html}
}

@article{joulin_bag_2016,
	title = {Bag of Tricks for Efficient Text Classification},
	url = {http://arxiv.org/abs/1607.01759},
	abstract = {This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier {fastText} is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train {fastText} on more than one billion words in less than ten minutes using a standard multicore{\textasciitilde}{CPU}, and classify half a million sentences among{\textasciitilde}312K classes in less than a minute.},
	journaltitle = {{arXiv}:1607.01759 [cs]},
	author = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
	urldate = {2020-03-15},
	date = {2016-08-09},
	eprinttype = {arxiv},
	eprint = {1607.01759},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/midles/Zotero/storage/U6TA5ZRK/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf:application/pdf;arXiv.org Snapshot:/home/midles/Zotero/storage/GPZSWWYA/1607.html:text/html}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pretrained on a large text corpus. We show that these representations can be easily added to existing models and signiﬁcantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	journaltitle = {{arXiv}:1802.05365 [cs]},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	urldate = {2020-03-15},
	date = {2018-03-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.05365},
	keywords = {Computer Science - Computation and Language},
	file = {Peters et al. - 2018 - Deep contextualized word representations.pdf:/home/midles/Zotero/storage/Q3992EAL/Peters et al. - 2018 - Deep contextualized word representations.pdf:application/pdf}
}